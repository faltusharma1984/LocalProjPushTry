File system |
DBMS		|
RDBMS		|
NoSQL		|
		   \ /
						____________		
Earlier Liner Storage  |_|__|_________| Data is stored sequentially.. We refer as tape

Then Disk come in to picture   ( Circle)  Randon access to any record - it is fast as compared to tape..provided capability to look any record 
 -two factors: seek(IN past 30 yrs improvement is less),  transfer(transfer rate has increaseed 100Mbs for disk to memory.

so for any huge data or big data operations ..seek time determines how fast the operation can be done.. siince seek time is not improved in past 30 yrs.
						____ 1) Storing Data ->DFS->HDFS
					/
Huge data growth  -
					\	____2) Processing Data -> Hadoop MapReduce

So haddop is good for Store large data (PB) , process on huge files. Limitation is it can do batch processing. 

-> data can look only in sequentinal manner.. & entrie dataset has to be looked even for simplest job( if requirement is to look for couple of records)
														 (  . . .)
--> In Hadoop they have not used the cocept of random seek( . . .  ) becoz seek is the slowest factor.. To process huge file it would take lot of time by OS to seek. so they came up with "Disk as tape" concept ie blocks .. increased the block size to 64Mb/128Mb. Inside block its reads in sequntial manner.

->so Here People want change that they dont want thier huge data should should not be looked into sequential manner... they want sthing like RDBMS (Random retreival).. fast retreival... any point of time data in O(1) complexity.. look up  time. This is where projects came like Cassandra, Hbase, MongoDB, DynomoDB, couchDB
it means not only sotre	huge data but also lookup data in Random fashion.				
				



 
 
	
				 
-----------------Hbase Concepts----------

--Hbase is Distributed data store 
--Apache open source
--Horizontal scalable; we can add simply more machines to pool of REsources- we can add REgion serveres on need basis & data can split over to other machines(RegionServers). And verticle scalable means adding more power(RAM, CPU) to your existing machine.
--Sorted Map -

How RDBMS works-  1000 records -  how it is able to retrieve  as single record.. internally  it has implemented  B+ Tree algo to search a key.  B+Tree is good for Random data becoz data has to go for seeks..& they  have concepts of index as well

--Here in Hadoop we store the data in sequential manner.   

HashMap is good concept for storing key value pairs & searching keys..but hashmap has limitation .. it is based on memory ..RAM memory limits to(max size 64 GB)..

->In Hbase we are using concept of  HashMap ..which is sorted	hashmap.. but here is Distributed Hashmap.. for 40yrs people has researched(DHT-Distributed hash table) - Google has found that for Distributed hash table- we need distributed file system.
K| V |
------
 |   |
 |   |
 |   |
 
Instead of storing all the keys in one table ( Million of recordso) it will stored in multiple hashtables. 
0-100 (keys in one table)
101-200 (keys in other table)
201-300 ..so on






//Horizontal scalable means :
---------------------------------
Horizontal scaling means that you scale by adding more machines into your pool of resources where Vertical scaling means that you scale by adding more power (CPU, RAM) to your existing machine.

In a database world horizontal-scaling is often based on partitioning of the data i.e. each node contains only part of the data , in vertical-scaling the data resides on a single node and scaling is done through multi-core i.e. spreading the load between the CPU and RAM resources of that machine.

With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool - Vertical-scaling is often limited to the capacity of a single machine, scaling beyond that capacity often involves downtime and comes with an upper limit.

A good example for horizontal scaling is Cassandra , MongoDB .. and a good example for vertical scaling is MySQL - Amazon RDS (The cloud version of MySQL) provides an easy way to scale vertically by switching from small to bigger machines this process often involves downtime.

In-Memory Data Grids such as GigaSpaces XAP, Coherence etc.. are often optimized for both horizontal and vertical scaling simply because they're not bound to disk. Horizontal-scaling through partitioning and vertical-scaling through multi-core support.

